# AI/ML Engineer â€” MLOps Pipeline (Project 01)

![Project 1 CI](https://github.com/MellowEgo/kyree-ai-portfolio/actions/workflows/ci.yaml/badge.svg)

> **One-line repo description:**  
> A production-style, config-driven ML pipeline that trains â†’ validates with thresholds â†’ serves via FastAPI with health/metrics â€” designed to mirror regulated enterprise workflows (e.g., Johnson & Johnson).

---

## ðŸŽ¯ Purpose

This project demonstrates a **production-grade ML pipeline** with disciplined engineering over flashy modeling.  
It highlights **reproducibility, traceability, and audit-friendly validation** â€” the same habits expected in regulated environments like **Johnson & Johnson**.

**What reviewers see in 60 seconds**
- `make check` runs a deterministic **train â†’ eval** loop and **fails** if metrics regress  
- `artifacts/metrics.json` + `artifacts/run_id.txt` prove **traceability**  
- `api/app.py` exposes **/health** and **/metrics** for **observability**  
- `tests/` includes **smoke + unit** tests for CI readiness  
- `configs/local.yaml` sets **thresholds** â€” no magic numbers

---

## ðŸ§© System Diagram

```mermaid
flowchart LR
    A[Data (Iris)] --> B[Train (scikit-learn)]
    B --> C[metrics.json + run_id.txt]
    C --> D[Eval: Threshold Check]
    D -->|pass| E[Serve (FastAPI)]
    D -->|fail| F[[Pipeline Fails with Exit 1]]
    E --> G[/health + /metrics]
    G --> H[Monitoring / CI]


![Project 1 CI](https://github.com/MellowEgo/kyree-ai-portfolio/actions/workflows/ci.yaml/badge.svg)

---

### Quickstart

> From **this folder** (`projects/01_ai_ml_engineer_mlopspipeline/`)

Run these commands from this folder
projects/01_ai_ml_engineer_mlopspipeline/

```bash
# 1) Install dependencies (uv) and set up local env
make setup

# 2) Run the end-to-end pipeline (train -> metrics -> threshold check)
make check

# 3) Serve health/metrics endpoints
make serve     # http://127.0.0.1:8010  (GET /health, GET /metrics)

# 4) Run tests (unit + smoke)
make test
```

Prereqs: Python 3.10+, uv (or swap uv run for your venv runner).
Artifacts: created under artifacts/ (gitignored).

### Try it quickly
```bash
# from this folder
uv run python src/train.py
uv run uvicorn --app-dir "api" app:app --reload


# in another terminal:
curl http://127.0.0.1:8000/health
curl -X POST http://127.0.0.1:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"features":[4,1]}'
```

### This project demonstrates an end-to-end ML workflow:
- Data preparation & feature engineering
- Model training & evaluation with thresholds
- FastAPI service for inference
- Automated tests & CI integration\
- Docker-ready structure for deployment

### Stack
- Python 3.12 (managed by uv)
- scikit-learn, pandas, numpy
- FastAPI + Uvicorn
- PyTest
- GitHub Actions CI
- (Optional) Docker for containerized serving

### Structure
```bash

01_ai_ml_engineer_mlopspipeline/
â”œâ”€ api/
â”‚  â””â”€ app.py                  # FastAPI service (/health, /predict)
â”œâ”€ src/
â”‚  â”œâ”€ train.py                # writes artifacts/metrics.json
â”‚  â””â”€ eval.py                 # reads metrics.json and checks thresholds
â”œâ”€ tests/
â”‚  â”œâ”€ test_api.py
â”‚  â”œâ”€ test_features.py
â”‚  â”œâ”€ test_train.py
â”‚  â”œâ”€ test_smoke.py
â”‚  â””â”€ test_metrics.py
â”œâ”€ configs/
â”‚  â””â”€ local.yaml              # evaluation thresholds
â”œâ”€ assets/
â”‚  â””â”€ fastapi-docs.png        # screenshot of /docs (optional)
â”œâ”€ artifacts/                 # generated outputs (gitignored)
â”œâ”€ Makefile
â””â”€ README.md

```
### Make Targets
```bash
make setup    # uv sync (install deps)
make train    # generate artifacts/metrics.json
make eval     # read metrics.json and enforce thresholds
make check    # full train + eval (CI-safe)
make serve    # run FastAPI locally
make test     # pytest suite (unit + smoke)
make lint     # optional: ruff format/lint check
make clean    # remove artifacts & caches
```
ðŸŸ¢ Tip: If artifacts/metrics.json doesnâ€™t exist, run make train first.
ðŸŸ¢ The API is optional â€” omit if youâ€™re focusing on pipeline behavior.

### Example Outputs 
artifacts/metrics.json
```json
{
  "accuracy": 0.91,
  "f1_macro": 0.90,
  "latency_ms": 43.5
}
```
configs/local.yaml 
```yaml 
eval:
  thresholds:
    accuracy: 0.85
    f1_macro: 0.85
```
run_id.txt
```bash
20251009T142015-7f3a9bcd
```
### HealthCare Context 
```bash
| Principle            | Implementation                             |
| -------------------- | ------------------------------------------ |
| **Reproducibility**  | Config-driven, seeded runs                 |
| **Traceability**     | Run ID + persisted metrics artifacts       |
| **Validation Gates** | Pipeline fails if thresholds unmet         |
| **Observability**    | `/health` + `/metrics` endpoints           |
| **Governance**       | `.env.example` pattern; no secrets in repo |
| **Auditability**     | Versioned configs & deterministic outputs  |
```

### Future Enhancements
- MLflow tracking for metrics & parameters
- ONNX packaging for portable inference
- Docker build for ECS/Fargate deployment
- Prometheus/Grafana monitoring dashboard
- Bedrock/LLM summarization endpoint (no PHI)


---
```yaml
### ðŸ’¡ Why this version

âœ… Clean, readable, and GitHub-formatted  
âœ… CI badge + diagram render correctly  
âœ… Narrative tied to J&Jâ€™s environment  
âœ… Keeps technical detail but avoids redundancy  
âœ… Recruiter or engineer can **grasp full system in 30â€“60 seconds**

---

Would you like me to follow this by generating the **assets/pipeline.mmd** and **configs/local.yaml** next, so everything renders and runs right away?
```